import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import math

# --- 1. Configuration ---
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
N_DATA = 10000        
N_EPOCHS = 5000       # Increased epochs for better learning
BATCH_SIZE = 512      
LR = 1e-3             

# Multi-Level Noise Schedule 
# Discrete noise levels, matching standard SGM practice
SIGMAS = torch.tensor([0.01, 0.05, 0.1, 0.2, 0.5, 0.75], device=DEVICE) 
N_SIGMAS = len(SIGMAS)

# Annealed Langevin Sampling Parameters
SAMPLING_STEPS_PER_LEVEL = 50000  # Set high enough for proper resolution, 1000 is a good starting point.
SAMPLING_STEP_SIZE = 0.00005     # Fixed step size (epsilon) for Langevin Dynamics

# --- 2. Data Distribution ---

def generate_data(n_samples):
    """Generates a 2D mixture of 2 Gaussians (p0(x0))."""
    centers = torch.tensor([
        [2.0, 2.0], [-2.0, -2.0]
    ], device=DEVICE)
    
    mode_indices = torch.randint(0, 2, (n_samples,), device=DEVICE)
    data = centers[mode_indices] + torch.randn(n_samples, 2, device=DEVICE) * 0.3
    return data

def get_true_score_and_data(x_0, sigma):
    """Calculates the noisy sample and the true target score for a given sigma."""
    # 1. Add noise scaled by sigma
    noise = torch.randn_like(x_0) * sigma
    x = x_0 + noise
    
    # 2. Target Score: ?x log p(x|x0) = - noise / sigma^2
    true_score = -noise / (sigma**2)
    
    return x, true_score

# --- 3. Score Model (S_theta(x, sigma)) ---

class ScoreModel2D(nn.Module):
    """
    Noise Conditional Score Network (NCSN).
    Takes input 'x' (2D) and 'sigma' (scalar/batch) and outputs the score (2D).
    """
    def __init__(self, input_dim=2, hidden_dim=128, output_dim=2):
        super().__init__()
        
        # Simple embedding for the scalar sigma
        self.sigma_embed = nn.Sequential(
            nn.Linear(1, hidden_dim),
            nn.LeakyReLU()
        )
        
        # Core network
        self.net = nn.Sequential(
            nn.Linear(input_dim + hidden_dim, hidden_dim), 
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x, sigma):
        batch_size = x.size(0)
        
        # Prepare sigma as a tensor of shape (Batch_Size, 1)
        if sigma.numel() == 1:
            # If it's a single value, create a tensor [batch_size, 1] filled with that value
            sigma_tensor = sigma.view(1).repeat(batch_size).view(-1, 1)
        else: 
            # If sigma is already a batch, ensure it's in the correct shape [batch_size, 1]
            sigma_tensor = sigma.view(-1, 1)

        # Embed sigma
        sigma_emb = self.sigma_embed(sigma_tensor)
        
        # Concatenate x and sigma embedding
        h = torch.cat([x, sigma_emb], dim=1)
        
        return self.net(h)

# --- 4. Visualization Functions ---

def visualize_vector_field(model, data_samples, title, sigma_level):
    """Plots the generated data and the learned score vector field for a specific sigma."""
    fig, ax = plt.subplots(figsize=(6, 6))
    
    x_coords = np.linspace(-5, 5, 20)
    y_coords = np.linspace(-5, 5, 20)
    X, Y = np.meshgrid(x_coords, y_coords)
    grid_points = torch.tensor(np.vstack([X.ravel(), Y.ravel()]).T, dtype=torch.float32, device=DEVICE)

    model.eval()
    with torch.no_grad():
        sigma_tensor = torch.tensor(sigma_level, dtype=torch.float32, device=DEVICE)
        batch_sigma = sigma_tensor.repeat(grid_points.size(0))
        scores_tensor = model(grid_points, batch_sigma)
        
    scores = scores_tensor.cpu().numpy()
    U = scores[:, 0].reshape(X.shape)
    V = scores[:, 1].reshape(Y.shape)
    
    ax.scatter(data_samples[:, 0], data_samples[:, 1], s=5, alpha=0.1, label='Data Samples', color='blue')
    
    # Plot Vector Field
    ax.quiver(X, Y, U, V, color='red', alpha=0.8, scale=100, label=f'Score Vectors (Sigma={sigma_level:.2f})') 
    
    ax.set_title(title)
    ax.set_xlim(-5, 5)
    ax.set_ylim(-5, 5)
    ax.set_xlabel("X-coordinate")
    ax.set_ylabel("Y-coordinate")
    ax.set_aspect('equal', adjustable='box')
    
    plt.show() 
    plt.close()

def visualize_sampling(trajectories, title):
    """Plots the path of samples generated by Annealed Langevin Dynamics."""
    fig, ax = plt.subplots(figsize=(6, 6))
    
    final_samples = trajectories[-1].cpu().numpy()
    ax.scatter(final_samples[:, 0], final_samples[:, 1], s=10, alpha=0.6, color='green', label='Final Samples')

    trajectories_np = torch.stack(trajectories).cpu().numpy()
    n_trajectories = 10
    
    # Only plot the first 10 trajectories for clarity
    for i in range(n_trajectories):
        ax.plot(trajectories_np[:, i, 0], trajectories_np[:, i, 1], alpha=0.5, linestyle='--', linewidth=0.5, color='gray')
        ax.plot(trajectories_np[0, i, 0], trajectories_np[0, i, 1], 'o', markersize=3, color='purple', label='Start Noise' if i == 0 else None)
    
    ax.set_title(title)
    ax.set_xlim(-5, 5)
    ax.set_ylim(-5, 5)
    ax.set_xlabel("X-coordinate")
    ax.set_ylabel("Y-coordinate")
    ax.set_aspect('equal', adjustable='box')
    plt.legend()
    
    plt.show() 
    plt.close()

# --- 5. Annealed Langevin Dynamics Sampling (Corrected) ---

def annealed_langevin_sampling(model, n_samples=1000):
    """
    Recovers the initial distribution by following the score S_theta(x, sigma) 
    using the corrected Annealed Langevin Dynamics update.
    """
    model.eval()
    
    # Start from pure noise scaled by the LARGEST sigma (x_T)
    x = torch.randn(n_samples, 2, device=DEVICE) * SIGMAS[-1]
    
    trajectories = [x.clone()]

    # Iterate through each noise level in descending order
    # SIGMAS is a tensor of noise levels, e.g., [1.0, 0.5, 0.2, 0.1, 0.05, 0.01]
    for sigma_idx, sigma in enumerate(SIGMAS.flip(dims=(0,))):
        
        # CRITICAL CORRECTION: Update the step size (alpha) for the current noise level
        # The step size should be proportional to the variance (sigma^2) and a fixed epsilon (SAMPLING_STEP_SIZE).
        alpha = SAMPLING_STEP_SIZE * (sigma**2)
        
        for step in range(SAMPLING_STEPS_PER_LEVEL):
            with torch.no_grad():
                # Get the score S_theta(x, sigma)
                score = model(x, sigma)
                
                # Drift Term (Score following)
                drift = alpha * score
                
                # Diffusion Term (Controlled Noise)
                # The noise variance is sqrt(2 * alpha * sigma**2) but since alpha already contains sigma^2,
                # we just use sqrt(2 * alpha) or a similar formulation for stability.
                # Here, we use a robust, scaled noise term.
                noise_scale = math.sqrt(2 * alpha) 
                noise = torch.randn_like(x) * noise_scale
                
                # Langevin Dynamics update step
                x = x + drift + noise
                
                # Record trajectories sparingly for visualization clarity
                if step % (SAMPLING_STEPS_PER_LEVEL // 100) == 0:
                    trajectories.append(x.clone())
            
    return trajectories

# --- 6. Main Execution ---

def main():
    # Generate the clean data (x0)
    clean_data = generate_data(N_DATA)
    
    # 1. Visualize Initial Data State
    print("\n--- Visualizing 1: Original Data Distribution ---")
    visualize_vector_field(ScoreModel2D().to(DEVICE), clean_data.cpu().numpy(), 
                          "1. Original Data Distribution (2 Modes) - Base Line", SIGMAS[0].item())

    # --- Training Phase (Multi-Level Denoising Score Matching) ---
    
    model = ScoreModel2D().to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR)
    
    print(f"\nStarting Multi-Level DSM Training on {DEVICE} for {N_EPOCHS} epochs over {N_SIGMAS} levels.")

    for epoch in range(N_EPOCHS):
        # Sample a batch index and a random sigma index for this iteration
        batch_indices = torch.randint(0, N_DATA, (BATCH_SIZE,), device=DEVICE)
        sigma_idx = torch.randint(0, N_SIGMAS, (1,), device=DEVICE)
        sigma = SIGMAS[sigma_idx]

        x_0_batch = clean_data[batch_indices]
        
        # Get the noisy sample (x) and the true target score
        x_batch, true_score = get_true_score_and_data(x_0_batch, sigma)

        optimizer.zero_grad()
        
        # Model predicts the score S_theta(x, sigma)
        predicted_score = model(x_batch, sigma)
        
        # DSM Loss: L2 error between predicted score and true target score
        loss = F.mse_loss(predicted_score, true_score)
        
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % (N_EPOCHS // 10) == 0:
            print(f"Epoch {epoch+1}/{N_EPOCHS}, DSM Loss: {loss.item():.6f} (Sigma={sigma.item():.2f})")

    print("Training Complete. Noise Conditional Score function learned.")

    # --- Application Phase (Sampling) ---

    # 2. Visualize Learned Score Field at Highest Noise Level (Rough Guide)
    print("\n--- Visualizing 2: Learned Score Field (S_theta, High Noise) ---")
    visualize_vector_field(model, clean_data.cpu().numpy(), 
                          "2. Learned Score Field (S_theta, High Noise)", SIGMAS[-1].item())

    # 3. Visualize Learned Score Field at Lowest Noise Level (Fine Guide)
    print("\n--- Visualizing 3: Learned Score Field (S_theta, Low Noise) ---")
    visualize_vector_field(model, clean_data.cpu().numpy(), 
                          "3. Learned Score Field (S_theta, Low Noise)", SIGMAS[0].item())


    # 4. Recover the Initial Distribution via Annealed Langevin Dynamics
    trajectories = annealed_langevin_sampling(model, n_samples=1000)
    
    # 5. Visualize the Sampling Trajectory and Final Result
    print("\n--- Visualizing 4: Annealed Langevin Sampling Result ---")
    visualize_sampling(trajectories, 
                       "4. Annealed Langevin Sampling Trajectory (Score Following)")

if __name__ == '__main__':
    main()
